# The Standard NLP Pipeline

![The Standard NLP Pipeline Flowchart](../../images/nlp_pipeline.svg)

Building an NLP application isn't just about feeding text into a model. It involves a systematic process known as the **NLP Pipeline**. While specific steps can vary based on the task, a typical pipeline includes the following stages:

## 1. Data Acquisition

This is the first and most crucial step. You need to gather raw textual data from various sources.

*   **Sources:** Web scraping, APIs (Twitter, Reddit), databases, PDF documents, or user input.
*   **Challenge:** Data is often messy, unstructured, and noisy.

## 2. Text Preprocessing (Cleaning & Normalization)

Raw text is rarely ready for analysis. This phase prepares the text by making it consistent and manageable.

*   **Tokenization:** Breaking text into smaller units (words/sentences).
*   **Lowercasing:** Converting "Hello" and "hello" to the same form.
*   **Noise Removal:** Removing HTML tags, emojis, and special characters.
*   **Stopword Removal:** Filtering out common words like "the", "is", "at".
*   **Stemming/Lemmatization:** Reducing words to their root form (e.g., "running" -> "run").

## 3. Feature Extraction (Vectorization)

Computers can't understand text; they only understand numbers. In this step, processed text is converted into numerical vectors.

*   **Bag of Words (BoW):** Counting word frequencies.
*   **TF-IDF:** Weighing words by their importance in a document relative to a corpus.
*   **Word Embeddings:** Dense vector representations (Word2Vec, GloVe, BERT) that capture semantic meaning.

## 4. Modeling

This is where the "learning" happens. You apply algorithms to the numerical features to perform specific tasks.

*   **Classical ML:** Naive Bayes, SVM, Logistic Regression (good for simple classification).
*   **Deep Learning:** RNNs, LSTMs, Transformers (good for complex tasks like translation or generation).

## 5. Evaluation

How good is your model? You need to measure its performance using standard metrics.

*   **Accuracy:** Overall correctness.
*   **Precision, Recall, F1-Score:** Crucial for imbalanced datasets.
*   **BLEU/ROUGE:** Specialized metrics for translation and summarization.

## 6. Deployment & Monitoring

The final step is making your model available to users and keeping an eye on it.

*   **API:** Wrapping the model in a REST API (e.g., using FastAPI or Flask).
*   **Monitoring:** Checking for "model drift" (when real-world data changes and the model's performance drops).
