# Feature Engineering for Text Data

Feature engineering is the process of transforming raw data into features that can be used to train a machine learning model. For text data, this means converting text into a numerical representation.

This section will cover some of the most common feature engineering techniques for text data.

## 1. Bag-of-Words (BoW)

The bag-of-words (BoW) model is a simple and effective way to represent text data. It works by creating a vocabulary of all the unique words in the text and then representing each document as a vector of word counts.

### Mathematical Derivation

Let $D$ be a collection of documents, and let $V$ be the vocabulary of all the unique words in $D$. The BoW representation of a document $d \in D$ is a vector $x$ of length $|V|$, where $x_i$ is the number of times that the $i$-th word in the vocabulary appears in the document.

### Python Example

```python
from sklearn.feature_extraction.text import CountVectorizer

documents = [
    "This is the first document.",
    "This document is the second document.",
    "And this is the third one.",
    "Is this the first document?",
]

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(documents)

print(vectorizer.get_feature_names_out())
print(X.toarray())
```

**Output:**
```
['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']
[[0 1 1 1 0 0 1 0 1]
 [0 2 0 1 0 1 1 0 1]
 [1 0 0 1 1 0 1 1 1]
 [0 1 1 1 0 0 1 0 1]]
```

## 2. TF-IDF

TF-IDF (Term Frequency-Inverse Document Frequency) is a more sophisticated way to represent text data. It takes into account the importance of each word in a document.

### Mathematical Derivation

TF-IDF is calculated as the product of two terms:

*   **Term Frequency (TF):** The number of times that a word appears in a document.
*   **Inverse Document Frequency (IDF):** A measure of how rare a word is in the collection of documents.

The TF-IDF of a word $w$ in a document $d$ is calculated as follows:

$$
\text{TF-IDF}(w, d) = \text{TF}(w, d) \times \text{IDF}(w)
$$

where:

$$
\text{TF}(w, d) = \frac{\text{Number of times that } w \text{ appears in } d}{\text{Total number of words in } d}
$$

$$
\text{IDF}(w) = \log\frac{\text{Total number of documents}}{\text{Number of documents that contain } w}
$$

### Solved Example

Let's consider a corpus of two documents:

*   **Document 1:** "The cat sat on the mat."
*   **Document 2:** "The dog chased the cat."

Let's calculate the TF-IDF score for the word "cat" in Document 1.

**Term Frequency (TF):**

The word "cat" appears 1 time in Document 1, and there are 6 words in Document 1.

$$
\text{TF}(\text{"cat"}, \text{Document 1}) = \frac{1}{6} = 0.167
$$

**Inverse Document Frequency (IDF):**

There are 2 documents in the corpus, and the word "cat" appears in 2 of them.

$$
\text{IDF}(\text{"cat"}) = \log\frac{2}{2} = \log(1) = 0
$$

**TF-IDF:**

$$
\text{TF-IDF}(\text{"cat"}, \text{Document 1}) = 0.167 \times 0 = 0
$$

Now, let's calculate the TF-IDF score for the word "mat" in Document 1.

**Term Frequency (TF):**

The word "mat" appears 1 time in Document 1, and there are 6 words in Document 1.

$$
\text{TF}(\text{"mat"}, \text{Document 1}) = \frac{1}{6} = 0.167
$$

**Inverse Document Frequency (IDF):**

There are 2 documents in the corpus, and the word "mat" appears in 1 of them.

$$
\text{IDF}(\text{"mat"}) = \log\frac{2}{1} = \log(2) = 0.301
$$

**TF-IDF:**

$$
\text{TF-IDF}(\text{"mat"}, \text{Document 1}) = 0.167 \times 0.301 = 0.050
$$

As you can see, the TF-IDF score for "mat" is higher than the TF-IDF score for "cat". This is because "mat" is a rarer word than "cat".

### Python Example

```python
from sklearn.feature_extraction.text import TfidfVectorizer

documents = [
    "This is the first document.",
    "This document is the second document.",
    "And this is the third one.",
    "Is this the first document?",
]

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(documents)

print(vectorizer.get_feature_names_out())
print(X.toarray())
```

**Output:**
```
['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']
[[0.         0.46979139 0.58028582 0.38408524 0.         0.
  0.38408524 0.         0.38408524]
 [0.         0.6876236  0.         0.28488359 0.         0.53864762
  0.28488359 0.         0.28488359]
 [0.51184851 0.         0.         0.26710379 0.51184851 0.
  0.26710379 0.51184851 0.26710379]
 [0.         0.46979139 0.58028582 0.38408524 0.         0.
  0.38408524 0.         0.38408524]]
```

## 3. Word Embeddings

Word embeddings are dense vector representations of words. They are able to capture the semantic meaning of words and their relationships to other words.

Some popular word embedding models include:

*   **Word2Vec**
*   **GloVe**
*   **FastText**

These models will be covered in more detail in Part 5 of this guide.
