# Language Modeling

Language modeling is the task of predicting the next word in a sequence of words. It is a fundamental task in NLP, and it is used in a wide variety of applications, such as machine translation, speech recognition, and text generation.

## N-gram Language Models

![N-gram Sliding Window Visualization](../../images/ngram_viz.svg)

An n-gram is a sequence of n words. An n-gram language model predicts the next word in a sequence by looking at the previous n-1 words.

### Mathematical Derivation

The probability of a sequence of words $w_1, w_2, \dots, w_m$ is given by:

$$
P(w_1, w_2, \dots, w_m) = \prod_{i=1}^{m} P(w_i | w_1, \dots, w_{i-1})
$$

An n-gram language model approximates this probability by making the following assumption:

$$
P(w_i | w_1, \dots, w_{i-1}) \approx P(w_i | w_{i-n+1}, \dots, w_{i-1})
$$

The probabilities are estimated from the counts of n-grams in a large corpus of text. For a bigram model (n=2), the probability of a word $w_i$ given the previous word $w_{i-1}$ is calculated as:

$$
P(w_i | w_{i-1}) = \frac{\text{count}(w_{i-1}, w_i)}{\text{count}(w_{i-1})}
$$

### Solved Example

Let's consider the following corpus:

*   "<s> I am Sam </s>"
*   "<s> Sam I am </s>"
*   "<s> I do not like green eggs and ham </s>"

Let's calculate the probability of the sentence "<s> I am Sam </s>" using a bigram model.

$$
P(\text{"<s> I am Sam </s>"}) = P(\text{I} | \text{<s>}) \times P(\text{am} | \text{I}) \times P(\text{Sam} | \text{am}) \times P(\text{</s>} | \text{Sam})
$$

First, we need to calculate the counts of the unigrams and bigrams in the corpus:

**Unigrams:**
*   `<s>`: 3
*   `I`: 3
*   `am`: 2
*   `Sam`: 2
*   `</s>`: 3
*   `do`: 1
*   `not`: 1
*   `like`: 1
*   `green`: 1
*   `eggs`: 1
*   `and`: 1
*   `ham`: 1

**Bigrams:**
*   `(<s>, I)`: 2
*   `(I, am)`: 2
*   `(am, Sam)`: 1
*   `(Sam, </s>)`: 1
*   `(Sam, I)`: 1
*   `(<s>, Sam)`: 1
*   `(am, </s>)`: 1
*   `(I, do)`: 1
*   `(do, not)`: 1
*   `(not, like)`: 1
*   `(like, green)`: 1
*   `(green, eggs)`: 1
*   `(eggs, and)`: 1
*   `(and, ham)`: 1
*   `(ham, </s>)`: 1

Now we can calculate the probabilities:

$$
P(\text{I} | \text{<s>}) = \frac{\text{count}(\text{<s>, I})}{\text{count}(\text{<s>})} = \frac{2}{3}
$$

$$
P(\text{am} | \text{I}) = \frac{\text{count}(\text{I, am})}{\text{count}(\text{I})} = \frac{2}{3}
$$

$$
P(\text{Sam} | \text{am}) = \frac{\text{count}(\text{am, Sam})}{\text{count}(\text{am})} = \frac{1}{2}
$$

$$
P(\text{</s>} | \text{Sam}) = \frac{\text{count}(\text{Sam, </s>})}{\text{count}(\text{Sam})} = \frac{1}{2}
$$

Finally, we can calculate the probability of the sentence:

$$
P(\text{"<s> I am Sam </s>"}) = \frac{2}{3} \times \frac{2}{3} \times \frac{1}{2} \times \frac{1}{2} = \frac{4}{36} = \frac{1}{9}
$$

### Python Example

This example shows how to train a simple bigram language model using NLTK.

```python
from nltk.util import ngrams
from nltk.probability import FreqDist, LidstoneProbDist

# Sample corpus
corpus = "<s> this is a sentence </s> <s> this is another sentence </s>"
tokens = corpus.split()

# Create bigrams
bigrams = list(ngrams(tokens, 2))

# Create a frequency distribution of the bigrams
fdist = FreqDist(bigrams)

# Create a probability distribution
# We use Lidstone smoothing to handle unseen n-grams
prob_dist = LidstoneProbDist(fdist, 0.1)

# Predict the next word
next_word = prob_dist.max_key()
print(f"The most likely next word after 'is' is: {prob_dist.max_key()}")
```

## Neural Language Models

Neural language models are a more modern approach to language modeling. They use neural networks to learn the probability distribution of words in a language.

Some popular neural language models include:

*   Recurrent Neural Networks (RNNs)
*   Long Short-Term Memory (LSTMs)
*   Transformers

These models will be covered in more detail in Part 5 of this guide.
