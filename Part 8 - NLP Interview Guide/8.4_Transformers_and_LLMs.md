# 8.4 Transformers & LLMs Interview Questions

## Q1: Explain the "Self-Attention" mechanism in Transformers.

**Answer:**
Self-Attention allows the model to look at other words in the input sequence to better understand the current word. It calculates a weighted sum of all words, where the weights determine how much focus to place on each word relative to the current one.

**Reasoning & Deep Dive:**
*   **Q, K, V:** Every word is projected into three vectors: Query (Q), Key (K), and Value (V).
*   **The Math:** $\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$.
    *   $QK^T$: Dot product determines similarity. If the Query for "it" matches the Key for "robot", the score is high.
    *   Softmax: Normalizes scores to probabilities (weights).
    *   Weighted Sum: We multiply weights by Values to get the final representation.
*   **Result:** The representation of "it" in "The robot crashed because it was old" absorbs information primarily from "robot".

---

## Q2: What is "Multi-Head Attention"? Why do we need it?

**Answer:**
Multi-Head Attention runs the self-attention mechanism multiple times in parallel (each with different weight matrices). The outputs are concatenated and projected linearly.

**Reasoning & Deep Dive:**
*   **Different Perspectives:** A single attention head might focus on syntactic relationships (Subject-Verb). Another might focus on semantic relationships (Coreference).
*   **Analogy:** It's like looking at an object through different color filters. One head sees the grammar, one sees the context, one sees the entity relationships. This enriches the model's understanding capability.

---

## Q3: What is the difference between BERT and GPT?

**Answer:**
*   **BERT (Bidirectional Encoder Representations from Transformers):** It is an **Encoder-only** model. It uses Masked Language Modeling (MLM) to learn context from both left and right directions simultaneously. It is best for **understanding** tasks (Classification, NER, QA).
*   **GPT (Generative Pre-trained Transformer):** It is a **Decoder-only** model. It uses Causal Language Modeling (predict next word) and can only look at past tokens (left-to-right). It is best for **generation** tasks.

**Reasoning & Deep Dive:**
*   **Architecture:** BERT attends to *all* tokens. GPT applies a mask so token $T$ cannot attend to $T+1$.
*   **Objective:** BERT tries to fill in the blank (`The [MASK] sat on the mat`). GPT tries to write the story (`The cat sat...`).

---

## Q4: What is "Positional Encoding" and why do Transformers need it?

**Answer:**
Transformers process all tokens in parallel (unlike RNNs which are sequential). Without Positional Encoding, the model would view "The dog bit the man" and "The man bit the dog" as identical "Bag of Words" representations. Positional Encodings inject information about the *order* of tokens into the embeddings.

**Reasoning & Deep Dive:**
*   **Sinusoidal Functions:** The original paper used sine and cosine functions of different frequencies.
    *   $PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$ 
*   **Why Sine/Cosine?** It allows the model to potentially learn to attend by relative positions (e.g., "pay attention to the word 3 steps back").

---

## Q5: Explain "Transfer Learning" in the context of NLP.

**Answer:**
Transfer Learning involves training a model on a massive, general dataset (Pre-training) and then updating its weights on a smaller, specific dataset (Fine-tuning).

**Reasoning & Deep Dive:**
*   **Pre-training:** The model reads the entire internet (Common Crawl) to learn grammar, facts, and reasoning. It learns "what language is."
*   **Fine-tuning:** We add a small classification layer on top and train on 5,000 legal documents. The model reuses its vast linguistic knowledge to understand legal jargon quickly. This democratized NLP, as you no longer need millions of labeled examples for every specific task.

---

## Q6: What is "Masked Language Modeling" (MLM)?

**Answer:**
MLM is the pre-training objective used by BERT. We randomly hide (mask) some percentage of the input tokens (e.g., 15%) and force the model to predict the original words based on the context provided by the non-masked words.

**Reasoning & Deep Dive:**
*   **Bidirectionality:** Because the model can see the words *after* the mask, it learns bidirectional context.
*   **Efficiency:** The downside is that the model only learns from 15% of the tokens per batch, making it slower to train than GPT, which predicts every single token.

---

## Q7: What are the scaling laws for LLMs?

**Answer:**
Scaling laws observe that model performance (loss) improves predictably as a power-law function of three factors:
1.  **Model Size** (Parameters)
2.  **Dataset Size** (Tokens)
3.  **Compute** (FLOPs)

**Reasoning & Deep Dive:**
*   **Chinchilla Optimization:** Ideally, model size and data size should scale equally. Doubling the model size requires doubling the training data to be compute-optimal.
*   **Implication:** Simply making models bigger without enough data leads to diminishing returns (and overfitting).

---

## Q8: What is RLHF (Reinforcement Learning from Human Feedback)?

**Answer:**
RLHF is a fine-tuning method used to align LLMs with human intent (e.g., helpfulness, safety).

**Reasoning & Deep Dive:**
1.  **SFT (Supervised Fine-Tuning):** Train the model on high-quality Q&A pairs.
2.  **Reward Model:** Humans rank multiple model outputs (A is better than B). Train a "Reward Model" to predict human preference.
3.  **PPO (Proximal Policy Optimization):** Use Reinforcement Learning to optimize the LLM to maximize the score from the Reward Model.
*   *Why?* It's easier for humans to recognize a good answer than to write one.

---

## Q9: How do you handle context length limitations in Transformers?

**Answer:**
Standard Transformers (e.g., BERT) have a limit (e.g., 512 tokens) due to the $O(N^2)$ complexity of self-attention.

**Reasoning & Deep Dive:**
**Strategies:**
1.  **Truncation:** Just cut off the text (bad for long docs).
2.  **Sliding Window:** Process chunks and aggregate results.
3.  **Sparse Attention:** Models like Longformer or BigBird use attention patterns (local + global) to reduce complexity to $O(N)$.
4.  **RoPE/ALiBi:** Modern relative positional encodings (like in LLaMA) extrapolate better to longer sequences than trained absolute embeddings.

---

## Q10: What is "Parameter-Efficient Fine-Tuning" (PEFT) and LoRA?

**Answer:**
Fine-tuning a 175B parameter model is prohibitively expensive. PEFT methods update only a small subset of parameters. **LoRA (Low-Rank Adaptation)** is the most popular method.

**Reasoning & Deep Dive:**
*   **LoRA:** Instead of updating the full weight matrix $W$, we freeze $W$ and add trainable low-rank matrices $A$ and $B$ such that $\Delta W = A \times B$.
*   **Impact:** We might only train 0.1% of the total parameters, reducing VRAM usage drastically (allowing fine-tuning on consumer GPUs) while achieving performance comparable to full fine-tuning.
