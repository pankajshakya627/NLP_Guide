# 8.2 Classical ML & Text Processing Interview Questions

## Q1: Explain TF-IDF. Why do we use it over simple Bag-of-Words (BoW)?

**Answer:**
TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic used to reflect the importance of a word to a document in a collection or corpus. Unlike BoW, which only counts occurrences, TF-IDF reduces the weight of common words and increases the weight of rare words.

**Reasoning & Deep Dive:**
*   **BoW Flaw:** In a BoW model, the word "the" might appear 100 times, while "quantum" appears 2 times. The model might think "the" is the most important feature.
*   **The Math:**
    *   **TF (Term Frequency):** How often word $w$ appears in document $d$. High frequency = likely relevant.
    *   **IDF (Inverse Document Frequency):** $\log(\frac{N}{DF_t})$. If a word appears in *all* documents ($DF_t \approx N$), the ratio is 1, and log(1) is 0. The term is penalized to zero. If it appears in only one document, the value is high.
*   **Result:** A word like "quantum" gets a high score because it appears frequently in *this* document (high TF) but rarely elsewhere (high IDF).

---

## Q2: How does the Naive Bayes classifier work for Text Classification? Why is it called "Naive"?

**Answer:**
Naive Bayes is a probabilistic classifier based on Bayes' Theorem. It calculates the probability of a label given a set of words. It is called "Naive" because it makes the strong assumption that the features (words) are **conditionally independent** given the class label.

**Reasoning & Deep Dive:**
*   **The Assumption:** It assumes that the presence of the word "Artificial" is independent of the word "Intelligence". In reality, this is false; they often appear together ("Artificial Intelligence").
*   **Why it works anyway:** Despite the flawed assumption, Naive Bayes is incredibly effective for text (especially spam filtering) because we usually care about the *ranking* of classes (is P(Spam) > P(Ham)?), not the exact probability values.
*   **Efficiency:** It is computationally very fast and requires little training data compared to deep learning models.

---

## Q3: What is "n-gram" in text processing? What are the pros and cons of using higher 'n'?

**Answer:**
An n-gram is a contiguous sequence of *n* items from a given sample of text.
*   n=1: Unigram ("New", "York")
*   n=2: Bigram ("New York")
*   n=3: Trigram ("New York City")

**Reasoning & Deep Dive:**
*   **Pros of higher 'n':** Captures context and local word order. "Not happy" (bigram) has a different sentiment than "Not" and "happy" treated separately.
*   **Cons of higher 'n':**
    *   **Data Sparsity:** As *n* increases, the probability of seeing that exact sequence drops drastically. You need massively more data to train.
    *   **Dimensionality Explosion:** The vocabulary size grows exponentially. This leads to the "Curse of Dimensionality" for Bag-of-Words models.

---

## Q4: Explain Precision, Recall, and F1-Score. Why not just use Accuracy for NLP?

**Answer:**
Accuracy is often misleading in NLP because datasets are usually **imbalanced**.
*   **Precision:** Of all the instances predicted as positive, how many were actually positive? (Quality)
*   **Recall:** Of all the actual positive instances, how many did we find? (Coverage)
*   **F1-Score:** The harmonic mean of Precision and Recall.

**Reasoning & Deep Dive:**
*   **Scenario:** A Spam detection dataset has 99% legitimate emails and 1% spam.
*   **Accuracy Trap:** A "dumb" model that predicts *everything* as legitimate will have 99% accuracy but 0% recall on spam. It is useless.
*   **Trade-off:**
    *   High Precision is needed when false positives are costly (e.g., flagging a vital legal email as spam).
    *   High Recall is needed when false negatives are costly (e.g., missing a terrorist threat in intelligence monitoring).

---

## Q5: What is "Smoothing" (e.g., Laplace/Additive Smoothing) in Naive Bayes?

**Answer:**
Smoothing is a technique used to handle the problem of **zero probability** in probabilistic models. If a word (e.g., "Covid") never appeared in the "Spam" training data, the standard probability $P(	ext{"Covid"}|	ext{Spam})$ would be 0. Since probabilities are multiplied, this single zero would zero out the entire prediction.

**Reasoning & Deep Dive:**
*   **Laplace Smoothing:** We add a small count (usually 1) to every word count.
*   **Formula:** $P(w|c) = \frac{\text{count}(w, c) + 1}{\text{count}(c) + |V|}$
*   This ensures that no event has a probability of zero, allowing the model to make predictions even for unseen words based on the other words in the document.

---

## Q6: What is Latent Semantic Analysis (LSA)?

**Answer:**
LSA is an unsupervised technique used to discover hidden (latent) relationships between words and documents. It uses **Singular Value Decomposition (SVD)** to reduce the dimensionality of the TF-IDF matrix.

**Reasoning & Deep Dive:**
*   **The Intuition:** Words that occur in similar contexts tend to have similar meanings. LSA compresses the data so that synonyms (e.g., "car" and "automobile") map to similar vectors in the reduced space.
*   **Limitations:** It assumes that words form a Gaussian distribution (which they don't) and it cannot handle polysemy (multiple meanings) very well compared to modern embeddings.

---

## Q7: How do you handle "Out of Vocabulary" (OOV) words in classical NLP?

**Answer:**
OOV words are words encountered during testing that were not present during training.
In classical NLP (BoW/TF-IDF), these are typically ignored or mapped to a special `<UNK>` (Unknown) token.

**Reasoning & Deep Dive:**
*   **The Issue:** If your model creates a feature vector of size 10,000 (top 10k words), and the test sentence contains the word "crypto" which wasn't in the top 10k, the model literally cannot "see" it.
*   **Solutions:**
    1.  **Ignore:** Simply drop the word.
    2.  **UNK Token:** Replace rare words with `<UNK>` during training so the model learns how to handle "unknown" concepts.
    3.  **Character N-grams:** Use features based on characters (e.g., 'ing', 'ed') to capture morphological clues even if the full word is new.

---

## Q8: What is Cosine Similarity and why is it preferred over Euclidean Distance for text?

**Answer:**
Cosine Similarity measures the cosine of the angle between two non-zero vectors. It measures orientation, not magnitude. Euclidean distance measures the straight-line distance between points.

**Reasoning & Deep Dive:**
*   **Magnitude Issue:** Document A might be "AI is great". Document B might be "AI is great" repeated 100 times.
*   **Euclidean:** The distance between A and B is massive because B's vector magnitude is huge. They seem unrelated.
*   **Cosine:** The angle between them is 0 (they point in the exact same direction). Cosine similarity says they are identical (score of 1.0), which is semantically correct for most NLP tasks (topic matching).

---

## Q9: What is Text Normalization?

**Answer:**
Text normalization is the process of transforming text into a canonical (standard) form.

**Reasoning & Deep Dive:**
Examples include:
1.  **Lowercasing:** "Apple" -> "apple".
2.  **Number replacement:** "1989" -> "NUM".
3.  **Expanding contractions:** "can't" -> "cannot".
4.  **Unicode normalization:** converting different quote marks or accents to a standard format.
*Why?* It reduces the vocabulary size significantly, allowing models to generalize better. "Apple" and "apple" shouldn't be learned as two totally different concepts.

---

## Q10: What is the "Curse of Dimensionality" in Bag-of-Words?

**Answer:**
In BoW, every unique word in the vocabulary adds a new dimension/feature. A vocabulary of 50,000 words results in feature vectors of length 50,000.

**Reasoning & Deep Dive:**
*   **Sparse Data:** Most documents only contain a few hundred words. The vectors are 99.9% zeros.
*   **Data Requirement:** As dimensions increase, the volume of data needed to reliably learn patterns grows exponentially. Models struggle to find the "signal" amidst the vast empty space.
*   **Solution:** Dimensionality reduction (LSA) or Dense Embeddings (Word2Vec).
