# 8.1 General NLP & Linguistics Interview Questions

## Q1: What is the difference between NLP, NLU, and NLG?

**Answer:**
NLP (Natural Language Processing) is the superset field concerned with the interaction between computers and human language. NLU (Natural Language Understanding) and NLG (Natural Language Generation) are subsets. NLU focuses on reading and comprehension (extracting meaning), while NLG focuses on writing and production (creating text from data).

**Reasoning & Deep Dive:**
Think of it as a pipeline.
1.  **NLP (The whole box):** The engineer building the system.
2.  **NLU (Input Processing):** The system receives "Book a flight to Paris." NLU breaks this down: Intent="Book Flight", Destination="Paris". It deals with ambiguity, syntax, and semantics.
3.  **NLG (Output Processing):** The system decides to respond. It has structured data: `{"response": "confirm", "time": "10 AM"}`. NLG converts this to "I have confirmed your flight for 10 AM."

---

## Q2: What is the difference between Stemming and Lemmatization? Which one is better?

**Answer:**
Stemming is a crude heuristic process that chops off the ends of words to hopefully achieve a root form (e.g., "running" -> "run", but also "better" -> "bet"). Lemmatization is a linguistic process that uses a dictionary and morphological analysis to return the actual base word, or lemma (e.g., "better" -> "good").

**Reasoning & Deep Dive:**
*   **Stemming (e.g., Porter Stemmer):** Fast but aggressive. It operates on rules like "if ends in 'ing', remove 'ing'". This leads to **over-stemming** (stripping too much) or **under-stemming**. It often produces non-words ("univers" instead of "university").
*   **Lemmatization (e.g., WordNet Lemmatizer):** Slower but accurate. It requires Part-of-Speech (POS) tags to work correctly. For example, the word "saw" is lemmatized to "see" if it's a verb, but remains "saw" if it's a noun (the tool).
*   **Which is better?** Use Lemmatization for tasks requiring high precision (e.g., Question Answering, Chatbots). Use Stemming for tasks where speed is critical and noise is acceptable (e.g., massive search indexing).

---

## Q3: Explain the concept of "Stop Words". Should we always remove them?

**Answer:**
Stop words are high-frequency words (like "the", "is", "and", "in") that carry very little unique semantic meaning. Traditionally, they are removed to reduce the dataset size and noise. However, we should **not** always remove them.

**Reasoning & Deep Dive:**
*   **When to remove:** In "Bag-of-Words" or simple topic modeling, stop words dominate the frequency counts and drown out rare, meaningful words. Removing them improves the signal-to-noise ratio.
*   **When to KEEP:** In modern Deep Learning (LSTMs, BERT, GPT), stop words provide critical **grammatical structure** and **context**.
    *   *Example:* "To be or not to be." If you remove stop words, you are left with nothing.
    *   *Example:* "Flights to Paris" vs. "Flights from Paris". "To" and "from" are stop words, but removing them destroys the meaning of the query.

---

## Q4: What is Tokenization? What are the challenges associated with it?

**Answer:**
Tokenization is the process of breaking a stream of text into smaller chunks called tokens (words, subwords, or characters). It is the first step in almost any NLP pipeline.

**Reasoning & Deep Dive:**
It sounds simple ("just split by space"), but it is surprisingly hard:
1.  **Punctuation:** Is "U.S.A." three tokens or one? Is "don't" one token or two ("do", "n't")?
2.  **Hyphens:** Is "state-of-the-art" one concept or four words?
3.  **Language differences:**
    *   *Chinese/Japanese:* No spaces between words. You need complex algorithms just to find word boundaries.
    *   *German:* Highly agglutinative (words glued together). "Donaudampfschifffahrtskapitänsmütze" is one word but represents a whole sentence's worth of meaning.
4.  **Social Media:** Hashtags (#NLP), mentions (@user), and URLs need special handling.

---

## Q5: What is "Part-of-Speech (POS) Tagging" and why is it useful?

**Answer:**
POS Tagging is the process of assigning a grammatical category (Noun, Verb, Adjective, etc.) to every token in a text corpus based on its definition and context.

**Reasoning & Deep Dive:**
*   **Disambiguation:** The word "book" can be a Noun ("read a book") or a Verb ("book a flight"). POS tagging tells the downstream model which one it is.
*   **Feature Engineering:** In Named Entity Recognition (NER), nouns are much more likely to be entities than verbs. POS tags serve as strong features for NER systems.
*   **Lemmatization:** As mentioned in Q2, lemmatizers need POS tags to know how to reduce a word (e.g., is "meeting" a verb or a noun?).

---

## Q6: What is the "Ambiguity" problem in NLP? Give examples.

**Answer:**
Ambiguity is the inherent uncertainty of meaning in natural language. It occurs when a word, phrase, or sentence can be interpreted in multiple ways.

**Reasoning & Deep Dive:**
Types of Ambiguity:
1.  **Lexical Ambiguity:** A single word has multiple meanings.
    *   *Ex:* "I went to the **bank**." (River bank or financial bank?)
2.  **Syntactic Ambiguity:** The sentence structure allows multiple parse trees.
    *   *Ex:* "I saw the man with the telescope." (Did I have the telescope, or did the man have it?)
3.  **Semantic Ambiguity:** The meaning is unclear even if the structure is known.
    *   *Ex:* "He painted the car on the lawn." (Was the car on the lawn when painted, or is the painting of a car that is on a lawn?)
4.  **Anaphoric Ambiguity:** Pronoun resolution issues.
    *   *Ex:* "The car hit the pole and it broke." (Did the car break or the pole?)

---

## Q7: What is Dependency Parsing vs. Constituency Parsing?

**Answer:**
These are two ways to analyze the grammatical structure of a sentence. **Constituency Parsing** breaks a sentence into nested sub-phrases (constituents) like Noun Phrases (NP) and Verb Phrases (VP). **Dependency Parsing** focuses on the relationships (dependencies) between individual words, typically labeling one word as the "head" and others as "dependents".

**Reasoning & Deep Dive:**
*   **Constituency (Phrase Structure):** Good for understanding the hierarchy.
    *   *Tree:* S -> NP VP.
    *   *Use case:* Grammar checking.
*   **Dependency:** Good for understanding relationships and semantic roles (Subject, Object).
    *   *Structure:* "gave" is the root. "John" is the subject (nsubj) of "gave". "Book" is the object (dobj).
    *   *Use case:* Information Extraction, Question Answering (Who gave what to whom?). Dependency parsing is generally faster and more widely used in modern NLP libraries like spaCy.

---

## Q8: What is "Corpus" and "Lexicon" in NLP?

**Answer:**
A **Corpus** (plural: Corpora) is a large, structured set of texts used for training models (e.g., the Wikipedia corpus, Common Crawl). A **Lexicon** is a collection of words and their associated meanings or knowledge, like a dictionary or a vocabulary list.

**Reasoning & Deep Dive:**
*   **Corpus:** Raw data. It's what you feed into a machine learning model to learn patterns.
*   **Lexicon:** Cured knowledge. For example, a "Sentiment Lexicon" might list words like "happy" (+1) and "sad" (-1). In the pre-deep learning era, lexicons were critical. Now, models learn these associations implicitly from the corpus.

---

## Q9: What is Zipf's Law?

**Answer:**
Zipf's Law is an empirical law stating that the frequency of any word is inversely proportional to its rank in the frequency table. The most frequent word will occur twice as often as the second most frequent word, three times as often as the third, and so on.

**Reasoning & Deep Dive:**
*   **Formula:** $Frequency \propto \frac{1}{Rank}$
*   **Implication for NLP:** Language follows a "Power Law" distribution. A tiny number of words (stop words) make up 50% of any text. A massive number of words ("tail words") appear very rarely.
*   **Challenge:** This creates the **"long tail" problem**. It's easy for models to learn "the" and "is", but very hard to learn rare words like "defenestration" because there are so few training examples for them. This drives the need for techniques like **subword tokenization** (Byte-Pair Encoding) to handle rare words.

---

## Q10: What is a Regular Expression (Regex) and why is it still relevant?

**Answer:**
Regex is a sequence of characters that specifies a search pattern. Despite the rise of AI, Regex remains crucial for the **preprocessing** and **post-processing** stages of NLP.

**Reasoning & Deep Dive:**
*   **Cleaning:** Removing HTML tags (`<[^>]+>`), extracting emails (`[\w\.-]+@[\w\.-]+`), or removing digits.
*   **Rule-based Extraction:** Extracting dates (DD/MM/YYYY) or phone numbers often requires rigid rules that probabilistic models might miss or hallucinate.
*   **Speed:** Regex is extremely fast compared to running a BERT model just to find a phone number.
