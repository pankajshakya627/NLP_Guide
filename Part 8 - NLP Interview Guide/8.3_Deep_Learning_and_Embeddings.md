# 8.3 Deep Learning & Embeddings Interview Questions

## Q1: What is Word2Vec? How does it differ from Bag-of-Words?

**Answer:**
Word2Vec is a predictive model that learns **dense** vector representations (embeddings) of words. Unlike Bag-of-Words (BoW), which creates sparse, discrete vectors based on counts, Word2Vec creates continuous vectors where semantically similar words are close to each other in vector space.

**Reasoning & Deep Dive:**
*   **BoW:** High dimensional (e.g., 50,000), Sparse (mostly zeros), No semantic meaning (distance between "cat" and "dog" is the same as "cat" and "car").
*   **Word2Vec:** Low dimensional (e.g., 300), Dense (continuous numbers), Captures semantics ("King" - "Man" + "Woman" $\approx$ "Queen").
*   **Architecture:** It uses a shallow neural network (1 hidden layer) trained to predict context words (Skip-gram) or the target word (CBOW).

---

## Q2: Explain the difference between CBOW and Skip-gram architectures in Word2Vec.

**Answer:**
*   **CBOW (Continuous Bag of Words):** Predicts the target word based on the surrounding context words.
*   **Skip-gram:** Predicts the surrounding context words given the target word.

**Reasoning & Deep Dive:**
*   **CBOW:** Faster to train. Better accuracy for frequent words. It smooths over the distributional information (averaging context).
*   **Skip-gram:** Slower. Better for small datasets. Much better at representing **rare words** because it treats each context-target pair as a distinct observation, giving rare words more opportunities to update the weights.

---

## Q3: What is "GloVe"? How is it different from Word2Vec?

**Answer:**
GloVe (Global Vectors) is an unsupervised learning algorithm for obtaining vector representations for words. While Word2Vec is a **predictive** model (neural network), GloVe is a **count-based** model.

**Reasoning & Deep Dive:**
*   **Matrix Factorization:** GloVe constructs a massive co-occurrence matrix (how often word $i$ appears with word $j$ in the entire corpus). It then factorizes this matrix to yield vectors.
*   **Global vs. Local:** Word2Vec relies on local window contexts. GloVe captures global statistics of the corpus. It combines the benefits of matrix factorization (LSA) and local context window methods.

---

## Q4: Why do we need RNNs (Recurrent Neural Networks) for NLP? Why not standard Feed-Forward Networks?

**Answer:**
Standard Feed-Forward Networks (FFNs) assume inputs are independent and fixed-size. Language is **sequential** and **variable-length**. The meaning of a word depends on the words before it ("I do not like..." changes the meaning of "like"). RNNs have a "memory" (hidden state) that persists information across time steps.

**Reasoning & Deep Dive:**
*   **Parameter Sharing:** In an RNN, the same weights are applied at every time step. This allows the model to learn that "dog" behaves like a noun whether it appears at position 1 or position 20.
*   **Sequence Handling:** RNNs process inputs one by one, updating their internal state, allowing them to theoretically handle sequences of infinite length.

---

## Q5: What is the "Vanishing Gradient Problem" in RNNs?

**Answer:**
During Backpropagation Through Time (BPTT), gradients are calculated by multiplying weight matrices repeatedly (chain rule). If the weights are small (< 1), the gradients shrink exponentially as they propagate back to earlier time steps.

**Reasoning & Deep Dive:**
*   **Consequence:** The model effectively "forgets" information from the beginning of the sequence. It can't learn long-term dependencies (e.g., matching a subject at the start of a paragraph to a verb at the end).
*   **Exploding Gradient:** The opposite happens if weights are > 1. This is solved by **Gradient Clipping**.
*   **Solution:** Architecture changes like LSTM and GRU.

---

## Q6: How does an LSTM (Long Short-Term Memory) solve the Vanishing Gradient problem?

**Answer:**
LSTMs introduce a **Cell State** ($C_t$) that runs straight down the entire chain with only minor linear interactions. Information can flow along it unchanged. They use **Gates** (Input, Forget, Output) to regulate what information is added or removed.

**Reasoning & Deep Dive:**
*   **The Additive Property:** The cell state update equation is roughly $C_t = C_{t-1} + \text{new\_input}$.
*   **Gradient Flow:** During backpropagation, the additive nature means the gradient acts more like a "superhighway." It doesn't get multiplied by the weight matrix $W$ repeatedly in the same way, preventing it from vanishing quickly. This allows LSTMs to capture dependencies over hundreds of time steps.

---

## Q7: Compare GRU vs. LSTM. Which one should you choose?

**Answer:**
*   **LSTM:** 3 Gates (Input, Forget, Output). Separate Cell State and Hidden State.
*   **GRU (Gated Recurrent Unit):** 2 Gates (Update, Reset). Merges Cell State and Hidden State.

**Reasoning & Deep Dive:**
*   **Performance:** They generally perform comparably.
*   **Speed:** GRU is computationally cheaper and faster to train because it has fewer parameters (tensor operations).
*   **Data:** GRU often works better on smaller datasets. LSTM is clearer and more robust for tasks requiring very long-term memory due to the explicit separate cell state.

---

## Q8: What are Bi-directional RNNs (BiRNNs)?

**Answer:**
BiRNNs process the input sequence in both directions: forward (start to end) and backward (end to start). The outputs of both are concatenated at each time step.

**Reasoning & Deep Dive:**
*   **Context:** In the sentence "He said **Teddy** bears are cute", knowing "bears" (future context) helps identify "Teddy" as a toy, not a person (Teddy Roosevelt). Standard RNNs only see the past. BiRNNs see the whole sequence context.
*   **Limit:** They cannot be used for causal tasks like real-time text generation (you can't see the future in real-time), but are excellent for classification and tagging.

---

## Q9: What is "Sequence-to-Sequence" (Seq2Seq) learning?

**Answer:**
Seq2Seq is an architecture used to map a fixed-length input sequence to a variable-length output sequence (or vice versa). It typically consists of two RNNs: an **Encoder** (processes input into a context vector) and a **Decoder** (generates output from that vector).

**Reasoning & Deep Dive:**
*   **The Bottleneck:** The standard Seq2Seq model relies on the Encoder to compress the entire meaning of the input sentence into a single, fixed-size vector (the final hidden state). This is hard for long sentences.
*   **Solution:** This limitation led to the invention of the **Attention Mechanism**.

---

## Q10: What is ELMo (Embeddings from Language Models)?

**Answer:**
ELMo creates **contextualized** word embeddings. Unlike Word2Vec, where "bank" always has the same vector, ELMo generates a different vector for "bank" depending on the sentence (river bank vs. money bank).

**Reasoning & Deep Dive:**
*   **Deep Bi-LSTM:** ELMo trains a deep bidirectional LSTM language model.
*   **Dynamic:** The embedding is a function of the entire internal state of the model. This was the bridge between static embeddings (GloVe) and Transformers (BERT).
