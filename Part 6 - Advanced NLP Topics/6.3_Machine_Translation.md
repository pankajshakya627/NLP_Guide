# Machine Translation

Machine translation is the task of automatically translating text from one language to another. It is one of the oldest and most challenging tasks in NLP.

## The History of Machine Translation

The history of machine translation can be divided into four main periods:

1.  **The Early Years (1950s-1960s):** The first machine translation systems were developed in the 1950s. These systems were based on simple rule-based approaches.
2.  **The Rule-Based Era (1970s-1980s):** During this period, more sophisticated rule-based machine translation systems were developed. These systems were able to achieve a higher level of accuracy than the early systems.
3.  **The Statistical Revolution (1990s-2000s):** The 1990s saw a shift from rule-based approaches to statistical methods. This was driven by the increasing availability of large amounts of text data and the development of machine learning algorithms.
4.  **The Deep Learning Era (2010s-Present):** The 2010s have been marked by the rise of deep learning, which has led to significant breakthroughs in machine translation.

## Neural Machine Translation (NMT)

![Seq2Seq Architecture with Attention](../../images/seq2seq_attention.svg)

Neural machine translation (NMT) is the current state-of-the-art for machine translation. NMT models are based on deep learning, and they are trained on large datasets of parallel text.

### How NMT Works

NMT models typically use an encoder-decoder architecture. The encoder reads the source sentence and encodes it into a vector of numbers. The decoder then takes the vector and generates the target sentence.

### Python Example

Below are implementations showing how to use the Hugging Face Transformers library for machine translation. We show the high-level `pipeline` API, and then specific examples for loading models in **TensorFlow** and **PyTorch**.

### Option 1: High-Level Pipeline (Framework Agnostic)

```python
from transformers import pipeline

# Load the translation pipeline
translator = pipeline("translation_en_to_de")

# Translate a piece of text
result = translator("My name is Wolfgang and I live in Berlin.")

print(result[0]['translation_text'])
```

### Option 2: TensorFlow/Keras Specific Loading

```python
from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer
import tensorflow as tf

model_name = "t5-small"

# Load Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load Model (TensorFlow version)
model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)

# Prepare Input
text = "translate English to German: My name is Wolfgang and I live in Berlin."
inputs = tokenizer(text, return_tensors="tf")

# Inference
outputs = model.generate(inputs["input_ids"])
decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(decoded_output)
```

### Option 3: PyTorch Specific Loading

```python
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import torch

model_name = "t5-small"

# Load Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load Model (PyTorch version)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Prepare Input
text = "translate English to German: My name is Wolfgang and I live in Berlin."
inputs = tokenizer(text, return_tensors="pt")

# Inference
with torch.no_grad():
    outputs = model.generate(inputs["input_ids"])
    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(decoded_output)
```

**Output:**
```
Mein Name ist Wolfgang und ich lebe in Berlin.
```

## Evaluation Metric: BLEU Score

The Bilingual Evaluation Understudy (BLEU) score is the most common metric for evaluating machine translation. It measures the overlap of n-grams between the candidate translation and one or more reference translations.

### Mathematical Derivation

The BLEU score is calculated as follows:

$$
\text{BLEU} = \text{BP} \times \exp\left(\sum_{n=1}^N w_n \log p_n\right)
$$

where:

*   **BP (Brevity Penalty):** Penalizes translations that are too short.
    $$
    \text{BP} = \begin{cases} 1 & \text{if } c > r \\ \exp(1 - r/c) & \text{if } c \le r \end{cases}
    $$
    where $c$ is the length of the candidate translation and $r$ is the length of the reference translation.
*   **$p_n$:** Modified n-gram precision.
    $$
    p_n = \frac{\sum_{C \in \{\text{Candidates}\}} \sum_{\text{n-gram} \in C} \text{Count}_{\text{clip}}(\text{n-gram})}{\sum_{C \in \{\text{Candidates}\}} \sum_{\text{n-gram} \in C} \text{Count}(\text{n-gram})}
    $$
*   **$w_n$:** Weight for each n-gram size (usually uniform, e.g., $1/N$).

### Solved Example

**Candidate (C):** "the cat sat on the mat"
**Reference (R):** "the cat is on the mat"

**1. Unigram Precision ($p_1$):**
*   Candidate words: {the: 2, cat: 1, sat: 1, on: 1, mat: 1} (Total: 6)
*   Reference words: {the: 2, cat: 1, is: 1, on: 1, mat: 1}
*   Matches (clipped):
    *   "the": min(2, 2) = 2
    *   "cat": min(1, 1) = 1
    *   "sat": min(1, 0) = 0
    *   "on": min(1, 1) = 1
    *   "mat": min(1, 1) = 1
*   Total matches = 5
*   $p_1 = 5/6 = 0.833$

**2. Bigram Precision ($p_2$):**
*   Candidate bigrams: {the cat: 1, cat sat: 1, sat on: 1, on the: 1, the mat: 1} (Total: 5)
*   Reference bigrams: {the cat: 1, cat is: 1, is on: 1, on the: 1, the mat: 1}
*   Matches (clipped):
    *   "the cat": 1
    *   "cat sat": 0
    *   "sat on": 0
    *   "on the": 1
    *   "the mat": 1
*   Total matches = 3
*   $p_2 = 3/5 = 0.6$

**3. Brevity Penalty (BP):**
*   Candidate length ($c$) = 6
*   Reference length ($r$) = 6
*   $c = r$, so $\text{BP} = 1$

**4. Final BLEU (using N=2 and equal weights $w_1=w_2=0.5$):**
$$
\text{BLEU} = 1 \times \exp(0.5 \log(0.833) + 0.5 \log(0.6))
$$
$$
\text{BLEU} = \exp(0.5 \times -0.182 + 0.5 \times -0.510)
$$
$$
\text{BLEU} = \exp(-0.091 - 0.255) = \exp(-0.346) \approx 0.707
$$

## Challenges in Machine Translation

Despite the significant progress that has been made in machine translation, there are still many challenges that need to be addressed. These challenges include:

*   **Ambiguity:** Human language is often ambiguous. This can make it difficult for machine translation systems to produce accurate translations.
*   **Out-of-vocabulary words:** Machine translation systems can have difficulty translating words that are not in their vocabulary.
*   **Domain adaptation:** Machine translation systems can have difficulty translating text that is from a different domain than the data that they were trained on.
*   **Evaluation:** It is difficult to evaluate the quality of machine translation systems.
