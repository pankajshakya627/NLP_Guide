# Part 4: Machine Learning for NLP

This section bridges the gap between text data and predictive modeling. We apply classical supervised learning algorithms to solve fundamental NLP tasks.

## ðŸŽ¯ Learning Objectives
By the end of this section, you will be able to:
*   Implement text classification using Naive Bayes, Logistic Regression, and SVMs.
*   Understand the math behind these classifiers.
*   Perform Sentiment Analysis on text data.
*   Extract entities (NER) and tag parts of speech (POS) using statistical models like HMMs.

## ðŸ“‚ Section Contents

### [4.1 Text Classification](4.1_Text_Classification.md)
The classic algorithms:
*   **Naive Bayes:** Probabilistic classification with a solved math example.
*   **Logistic Regression:** Classification using the logistic function.
*   **Support Vector Machines (SVM):** Finding the optimal hyperplane.

### [4.2 Sentiment Analysis](4.2_Sentiment_Analysis.md)
Determining the emotional tone of text:
*   Rule-based approaches.
*   Machine learning approaches.
*   Python example using `TextBlob`.

### [4.3 Named Entity Recognition (NER)](4.3_Named_Entity_Recognition.md)
Identifying real-world objects:
*   Extracting Names, Organizations, Locations, and Dates.
*   Python example using `spaCy`.

### [4.4 Part-of-Speech Tagging](4.4_Part-of-Speech_Tagging.md)
Assigning grammatical tags (Noun, Verb, Adjective):
*   **Math Spotlight:** **Hidden Markov Models (HMM)** and the **Viterbi Algorithm**. Includes a detailed, step-by-step walkthrough of the Viterbi trellis calculation.