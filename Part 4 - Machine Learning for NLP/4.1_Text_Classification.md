# Text Classification

Text classification is the task of assigning a category or label to a piece of text. It is one of the most common tasks in NLP, and it has a wide variety of applications, such as spam detection, sentiment analysis, and topic labeling.

This section will cover some of the most common machine learning algorithms for text classification.

## Training vs. Inference Workflow

Understanding the separation between training and inference is crucial for deploying text classifiers.

```mermaid
flowchart LR
    subgraph Training ["Training Phase"]
        direction TB
        A1[Labeled Dataset] --> B1[Preprocessing]
        B1 --> C1[Vectorization (Fit & Transform)]
        C1 --> D1[Model Training]
        D1 --> E1((Trained Model))
        C1 -.-> V((Saved Vocab))
    end

    subgraph Inference ["Inference Phase"]
        direction TB
        A2[New Unseen Text] --> B2[Preprocessing]
        B2 --> C2[Vectorization (Transform only)]
        V -.-> C2
        C2 --> E1
        E1 --> F2[Prediction / Class Label]
    end
```

## 1. Naive Bayes

Naive Bayes is a simple and effective algorithm for text classification. It is based on Bayes' theorem, which is a a theorem that describes the probability of an event, based on prior knowledge of conditions that might be related to the event.

### Mathematical Derivation

The probability of a document $d$ belonging to a class $c$ is given by:

$$
P(c|d) = \frac{P(d|c)P(c)}{P(d)}
$$

The Naive Bayes classifier makes the "naive" assumption that the features (i.e., the words in the document) are independent of each other. This assumption is not always true, but it simplifies the problem and often works well in practice.

With the naive assumption, the probability of a document given a class is:

$$
P(d|c) = \prod_{i=1}^{n} P(w_i|c)
$$

where $w_i$ is the i-th word in the document.

To classify a new document, we calculate the probability of the document belonging to each class and choose the class with the highest probability.

$$
\hat{c} = \arg\max_c P(c|d) = \arg\max_c P(c) \prod_{i=1}^{n} P(w_i|c)
$$

The probabilities $P(c)$ and $P(w_i|c)$ are estimated from the training data.

### Solved Example

Let's consider the following training data:

| Document | Text | Class |
|---|---|---|
| 1 | "A great game" | Sports |
| 2 | "The election was over" | Not Sports |
| 3 | "A very clean game" | Sports |
| 4 | "A great election" | Not Sports |

We want to classify the new document "A very great game".

First, we need to calculate the prior probabilities of each class:

$$
P(\text{Sports}) = \frac{2}{4} = 0.5
$$

$$
P(\text{Not Sports}) = \frac{2}{4} = 0.5
$$

Next, we need to calculate the conditional probabilities of each word given each class. We will use Laplace smoothing to avoid zero probabilities.

**For the "Sports" class:**

*   `P("A" | "Sports")` = (2 + 1) / (8 + 6) = 3/14
*   `P("great" | "Sports")` = (1 + 1) / (8 + 6) = 2/14
*   `P("game" | "Sports")` = (2 + 1) / (8 + 6) = 3/14
*   `P("very" | "Sports")` = (1 + 1) / (8 + 6) = 2/14

**For the "Not Sports" class:**

*   `P("A" | "Not Sports")` = (1 + 1) / (8 + 6) = 2/14
*   `P("great" | "Not Sports")` = (1 + 1) / (8 + 6) = 2/14
*   `P("election" | "Not Sports")` = (2 + 1) / (8 + 6) = 3/14
*   `P("was" | "Not Sports")` = (1 + 1) / (8 + 6) = 2/14
*   `P("over" | "Not Sports")` = (1 + 1) / (8 + 6) = 2/14
*   `P("The" | "Not Sports")` = (1 + 1) / (8 + 6) = 2/14

Now, we can calculate the probability of the new document belonging to each class:

$$
P(\text{Sports} | \text{"A very great game"}) \propto P(\text{Sports}) \times P(\text{"A"}|\text{Sports}) \times P(\text{"very"}|\text{Sports}) \times P(\text{"great"}|\text{Sports}) \times P(\text{"game"}|\text{Sports})
$$

$$
\propto 0.5 \times \frac{3}{14} \times \frac{2}{14} \times \frac{2}{14} \times \frac{3}{14} = 0.00032
$$

$$
P(\text{Not Sports} | \text{"A very great game"}) \propto P(\text{Not Sports}) \times P(\text{"A"}|\text{Not Sports}) \times P(\text{"very"}|\text{Not Sports}) \times P(\text{"great"}|\text{Not Sports}) \times P(\text{"game"}|\text{Not Sports})
$$

Since "very" and "game" do not appear in the "Not Sports" documents, their conditional probabilities are 1/14 with Laplace smoothing.

$$
\propto 0.5 \times \frac{2}{14} \times \frac{1}{14} \times \frac{2}{14} \times \frac{1}{14} = 0.00003
$$

Since $0.00032 > 0.00003$, we classify the new document as "Sports".

### Python Example

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

# Sample data
X = ["This is a positive review.", "This is a negative review.", "I am happy with this product.", "I am not happy with this product."]
y = ["positive", "negative", "positive", "negative"]

# Create a CountVectorizer to convert the text data to a matrix of token counts
vectorizer = CountVectorizer()
X_counts = vectorizer.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_counts, y, test_size=0.25, random_state=42)

# Create a Multinomial Naive Bayes classifier
clf = MultinomialNB()

# Train the classifier
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Evaluate the accuracy of the classifier
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Predict the sentiment of a new review
new_review = "This is a great product."
new_review_counts = vectorizer.transform([new_review])
prediction = clf.predict(new_review_counts)
print("Prediction:", prediction[0])
```

## 2. Logistic Regression

Logistic regression is another popular algorithm for text classification. It is a linear model that can be used for both binary and multiclass classification.

### Mathematical Derivation

Logistic regression models the probability that a document $d$ belongs to a class $c$ using the logistic function:

$$
P(c|d) = \frac{1}{1 + e^{-(\mathbf{w} \cdot \mathbf{x} + b)}}
$$

where:

*   $\mathbf{x}$ is the vector representation of the document (e.g., BoW or TF-IDF)
*   $\mathbf{w}$ is a vector of weights
*   $b$ is a bias term

The model is trained by finding the weights $\mathbf{w}$ and the bias $b$ that maximize the likelihood of the training data.

### Python Example
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Sample data
X = ["This is a positive review.", "This is a negative review.", "I am happy with this product.", "I am not happy with this product."]
y = ["positive", "negative", "positive", "negative"]

# Create a TfidfVectorizer to convert the text data to a matrix of TF-IDF features
vectorizer = TfidfVectorizer()
X_tfidf = vectorizer.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.25, random_state=42)

# Create a Logistic Regression classifier
clf = LogisticRegression()

# Train the classifier
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Evaluate the accuracy of the classifier
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Predict the sentiment of a new review
new_review = "This is a great product."
new_review_tfidf = vectorizer.transform([new_review])
prediction = clf.predict(new_review_tfidf)
print("Prediction:", prediction[0])

```

## 3. Support Vector Machines (SVM)

Support Vector Machines (SVM) are a powerful and versatile class of supervised machine learning algorithms. They can be used for both classification and regression tasks.

### Mathematical Derivation

The goal of an SVM is to find a hyperplane that separates the data into two classes with the maximum margin. The margin is the distance between the hyperplane and the closest data points from each class.

The hyperplane is defined by the equation:

$$
\mathbf{w} \cdot \mathbf{x} + b = 0
$$

where:

*   $\mathbf{w}$ is a vector of weights
*   $\mathbf{x}$ is the vector representation of the document
*   $b$ is a bias term

The SVM is trained by finding the weights $\mathbf{w}$ and the bias $b$ that maximize the margin.

### Python Example
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Sample data
X = ["This is a positive review.", "This is a negative review.", "I am happy with this product.", "I am not happy with this product."]
y = ["positive", "negative", "positive", "negative"]

# Create a TfidfVectorizer to convert the text data to a matrix of TF-IDF features
vectorizer = TfidfVectorizer()
X_tfidf = vectorizer.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.25, random_state=42)

# Create a a Support Vector Classifier
clf = SVC(kernel='linear')

# Train the classifier
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Evaluate the accuracy of the classifier
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Predict the sentiment of a new review
new_review = "This is a great product."
new_review_tfidf = vectorizer.transform([new_review])
prediction = clf.predict(new_review_tfidf)
print("Prediction:", prediction[0])
```
