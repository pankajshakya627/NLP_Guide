# Part-of-Speech (POS) Tagging

Part-of-speech (POS) tagging is the process of assigning a grammatical category (such as noun, verb, adjective, etc.) to each word in a piece of text.

POS tagging is a fundamental task in NLP, and it is used in a wide variety of applications, such as machine translation, speech recognition, and information retrieval.

## Approaches to POS Tagging

There are two main approaches to POS tagging:

1.  **Rule-based:** This approach uses a set of handcrafted rules to assign POS tags to words.
2.  **Stochastic:** This approach uses statistical models to learn to assign POS tags to words from a labeled dataset.

## Python Example

This example shows how to use the NLTK library to perform POS tagging on a piece of text.

```python
import nltk
from nltk.tokenize import word_tokenize

# Download the 'averaged_perceptron_tagger' resource
try:
    nltk.data.find('taggers/averaged_perceptron_tagger')
except nltk.downloader.DownloadError:
    nltk.download('averaged_perceptron_tagger')

text = "Natural Language Processing is a fascinating field."

tokens = word_tokenize(text)
tags = nltk.pos_tag(tokens)

print(tags)
```

**Output:**
```
[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('fascinating', 'JJ'), ('field', 'NN'), ('.', '.')]
```

### Explanation

The output is a list of tuples, where each tuple contains a word and its POS tag. The POS tags are from the Penn Treebank tagset. Here is a list of some of the most common POS tags:

*   **NNP:** Proper noun, singular
*   **NN:** Noun, singular or mass
*   **JJ:** Adjective
*   **VBZ:** Verb, 3rd person singular present
*   **DT:** Determiner
*   **.** : Punctuation

### Hidden Markov Models (HMM) and the Viterbi Algorithm

Most stochastic POS taggers use Hidden Markov Models (HMM). An HMM is a statistical model that assumes that the system being modeled is a Markov process with unobserved (hidden) states. In POS tagging, the hidden states are the POS tags, and the observed output are the words.

To find the most likely sequence of tags $Y = (y_1, y_2, \dots, y_n)$ for a given sequence of words $X = (x_1, x_2, \dots, x_n)$, we maximize the probability $P(Y|X)$. Using Bayes' theorem, this is equivalent to maximizing $P(Y)P(X|Y)$.

The HMM makes two assumptions:
1.  **Transition Probability:** The probability of a tag depends only on the previous tag. $P(y_i | y_{i-1}, \dots, y_1) \approx P(y_i | y_{i-1})$
2.  **Emission Probability:** The probability of a word depends only on its current tag. $P(x_i | y_1, \dots, y_n, x_1, \dots, x_n) \approx P(x_i | y_i)$

So, we want to maximize:
$$
\hat{Y} = \arg\max_Y \prod_{i=1}^n P(x_i | y_i) P(y_i | y_{i-1})
$$

The **Viterbi algorithm** is a dynamic programming algorithm used to find this most likely sequence efficiently.

### Solved Example: Viterbi Algorithm

**Sentence:** "fish swim"
**States (Tags):** Noun (N), Verb (V)

**Probabilities:**

*   **Initial Probabilities ($\pi$):**
    *   P(N | start) = 0.6
    *   P(V | start) = 0.4

*   **Transition Probabilities ($A$):**
    *   P(N | N) = 0.3
    *   P(V | N) = 0.7
    *   P(N | V) = 0.8
    *   P(V | V) = 0.2

*   **Emission Probabilities ($B$):**
    *   P("fish" | N) = 0.7
    *   P("fish" | V) = 0.3
    *   P("swim" | N) = 0.2
    *   P("swim" | V) = 0.8

**Step 1: Initialization (t=1, word="fish")**

We calculate the probability of starting with each tag and emitting "fish".

*   **V[1, N]** = P(N | start) * P("fish" | N) = 0.6 * 0.7 = **0.42**
*   **V[1, V]** = P(V | start) * P("fish" | V) = 0.4 * 0.3 = **0.12**

**Step 2: Recursion (t=2, word="swim")**

We find the maximum probability of reaching each tag at t=2 from the previous step.

*   **For Tag N:**
    *   From N: V[1, N] * P(N | N) * P("swim" | N) = 0.42 * 0.3 * 0.2 = 0.0252
    *   From V: V[1, V] * P(N | V) * P("swim" | N) = 0.12 * 0.8 * 0.2 = 0.0192
    *   **V[2, N]** = max(0.0252, 0.0192) = **0.0252** (Backpointer: N)

*   **For Tag V:**
    *   From N: V[1, N] * P(V | N) * P("swim" | V) = 0.42 * 0.7 * 0.8 = 0.2352
    *   From V: V[1, V] * P(V | V) * P("swim" | V) = 0.12 * 0.2 * 0.8 = 0.0192
    *   **V[2, V]** = max(0.2352, 0.0192) = **0.2352** (Backpointer: N)

**Step 3: Termination and Backtracking**

We find the highest probability at the last step.
*   max(V[2, N], V[2, V]) = max(0.0252, 0.2352) = 0.2352.
*   The best tag at t=2 is **V** (Verb).

Now, we backtrack using the backpointers:
*   The best tag at t=2 came from the backpointer stored for V[2, V], which is **N**.

**Result:**
The most likely tag sequence is **Noun, Verb**.
