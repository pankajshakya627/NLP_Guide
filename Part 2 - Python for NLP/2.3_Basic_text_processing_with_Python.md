# Basic Text Processing with Python

Text processing is a fundamental step in any NLP pipeline. It involves cleaning and preparing raw text data for analysis. This section will cover some of the most common text processing techniques using Python.

## 1. Tokenization

Tokenization is the process of breaking down text into smaller units, called tokens. Tokens can be words, sentences, or subwords.

### Word Tokenization

```python
import nltk
from nltk.tokenize import word_tokenize

# Download the 'punkt' resource
try:
    nltk.data.find('tokenizers/punkt')
except nltk.downloader.DownloadError:
    nltk.download('punkt')

text = "Natural Language Processing is a fascinating field."
tokens = word_tokenize(text)
print(tokens)
```

**Output:**
```
['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', '.']
```

### Sentence Tokenization

```python
import nltk
from nltk.tokenize import sent_tokenize

text = "Natural Language Processing is a fascinating field. It has many applications."
sentences = sent_tokenize(text)
print(sentences)
```

**Output:**
```
['Natural Language Processing is a fascinating field.', 'It has many applications.']
```

## 2. Lowercasing

Lowercasing is the process of converting all text to lowercase. This is important for ensuring that words like "Apple" and "apple" are treated as the same word.

```python
text = "Natural Language Processing is a fascinating field."
lowercase_text = text.lower()
print(lowercase_text)
```

**Output:**
```
natural language processing is a fascinating field.
```

## 3. Stopword Removal

Stopwords are common words that are often removed from text because they do not carry much meaning. Examples of stopwords include "the", "a", "an", and "in".

```python
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download the 'stopwords' resource
try:
    nltk.data.find('corpora/stopwords')
except nltk.downloader.DownloadError:
    nltk.download('stopwords')

text = "Natural Language Processing is a fascinating field."
stop_words = set(stopwords.words('english'))
tokens = word_tokenize(text)
filtered_tokens = [word for word in tokens if not word in stop_words]
print(filtered_tokens)
```

**Output:**
```
['Natural', 'Language', 'Processing', 'fascinating', 'field', '.']
```

## 4. Stemming and Lemmatization

Stemming and lemmatization are two techniques for reducing words to their root form.

### Stemming

Stemming is a crude heuristic process that chops off the ends of words.

```python
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

stemmer = PorterStemmer()
text = "running, runner, runs"
tokens = word_tokenize(text)
stemmed_tokens = [stemmer.stem(word) for word in tokens]
print(stemmed_tokens)
```

**Output:**
```
['run', ',', 'runner', ',', 'run']
```

### Lemmatization

Lemmatization is a more sophisticated process that uses a dictionary to reduce words to their base form.

```python
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Download the 'wordnet' resource
try:
    nltk.data.find('corpora/wordnet')
except nltk.downloader.DownloadError:
    nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()
text = "running, runner, runs"
tokens = word_tokenize(text)
lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]
print(lemmatized_tokens)
```

**Output:**
```
['running', ',', 'runner', ',', 'run']
```
As you can see, lemmatization is more accurate than stemming. It correctly identifies that "running" and "runs" are forms of the verb "run".
