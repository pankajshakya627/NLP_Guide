# Gated Recurrent Units (GRUs)

Gated Recurrent Units (GRUs) are a type of recurrent neural network (RNN) that is similar to LSTMs. GRUs were introduced in 2014 by Kyunghyun Cho et al.

## How GRUs Work

GRUs are similar to LSTMs, but they have a simpler architecture. GRUs have two gates: a reset gate and an update gate.

*   **Reset gate:** This gate controls which information from the previous hidden state is forgotten.
*   **Update gate:** This gate controls how much of the previous hidden state is passed on to the current hidden state.

### Mathematical Derivation

The gates and hidden state of a GRU at time step $t$ are calculated as follows:

$$ 
z_t = \sigma(W_{iz}x_t + b_{iz} + W_{hz}h_{t-1} + b_{hz})
$$

$$ 
r_t = \sigma(W_{ir}x_t + b_{ir} + W_{hr}h_{t-1} + b_{hr})
$$

$$ 
\tilde{h_t} = \tanh(W_{i\tilde{h}}x_t + b_{i\tilde{h}} + r_t \odot (W_{h\tilde{h}}h_{t-1} + b_{h\tilde{h}}))
$$

$$ 
h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h_t}
$$

where:

*   $x_t$ is the input at time step $t$
*   $h_{t-1}$ is the hidden state at the previous time step
*   $W$ and $b$ are weight matrices and bias vectors
*   $\sigma$ is the sigmoid activation function
*   $\tanh$ is the hyperbolic tangent activation function
*   $\odot$ is the element-wise product

## Python Example

Below are implementations of a simple GRU for text classification using both **TensorFlow/Keras** and **PyTorch**.

### Option 1: TensorFlow/Keras Implementation

```python
from keras.models import Sequential
from keras.layers import Embedding, GRU, Dense
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import numpy as np

# Sample data
texts = ["this is a positive review", "this is a negative review", "i am happy", "i am sad"]
labels = np.array([1, 0, 1, 0])

# Tokenize the text
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

# Pad the sequences
max_len = max([len(s) for s in sequences])
padded_sequences = pad_sequences(sequences, maxlen=max_len)

# Build the model
model = Sequential()
model.add(Embedding(len(tokenizer.word_index) + 1, 128, input_length=max_len))
model.add(GRU(64))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(padded_sequences, labels, epochs=10)

# Evaluate the model
loss, accuracy = model.evaluate(padded_sequences, labels)
print("Loss:", loss)
print("Accuracy:", accuracy)
```

### Option 2: PyTorch Implementation

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np

# Sample data
texts = ["this is a positive review", "this is a negative review", "i am happy", "i am sad"]
labels = np.array([1, 0, 1, 0])

# Preprocessing (Manual Tokenization for PyTorch example)
word2idx = {}
for text in texts:
    for word in text.split():
        if word not in word2idx:
            word2idx[word] = len(word2idx) + 1

sequences = []
for text in texts:
    sequences.append([word2idx[word] for word in text.split()])

# Pad sequences
max_len = max([len(s) for s in sequences])
padded_sequences = np.zeros((len(sequences), max_len), dtype=int)
for i, seq in enumerate(sequences):
    padded_sequences[i, :len(seq)] = seq

# Convert to Tensors
X_train = torch.tensor(padded_sequences, dtype=torch.long)
y_train = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)

# Dataset and DataLoader
dataset = TensorDataset(X_train, y_train)
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

# Build the Model
class GRUClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super(GRUClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        embedded = self.embedding(x)
        # GRU output: (batch, seq_len, hidden_dim)
        # hidden state: (num_layers, batch, hidden_dim)
        output, hidden = self.gru(embedded)
        
        # Use the final hidden state
        last_hidden = hidden[-1]
        return self.sigmoid(self.fc(last_hidden))

# Initialize Model
vocab_size = len(word2idx) + 1
embedding_dim = 128
hidden_dim = 64
model = GRUClassifier(vocab_size, embedding_dim, hidden_dim)

# Loss and Optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training Loop
for epoch in range(10):
    for inputs, targets in dataloader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
    
    if (epoch+1) % 2 == 0:
        print(f'Epoch [{epoch+1}/10], Loss: {loss.item():.4f}')

# Evaluation
with torch.no_grad():
    test_out = model(X_train)
    predicted = (test_out > 0.5).float()
    accuracy = (predicted == y_train).sum() / y_train.shape[0]
    print(f"Accuracy: {accuracy.item():.4f}")
```

## GRUs vs. LSTMs

GRUs are similar to LSTMs, but they have a simpler architecture. This makes them faster to train and less prone to overfitting. However, LSTMs have been shown to be more effective for some tasks.

```